{
  "hash": "4af000a58b3e96351e03f910f0fa1200",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"LME (Basic)\"\n---\n\n\n\n\n\n\n\nA **linear mixed-effects model (LME)** is a statistical model used to analyze data that have hierarchical or grouped structure. It combines two types of effects:\n\n1. **Fixed effects**: Represent relationships or effects that are constant across the entire population or dataset (e.g., the overall impact of a treatment or predictor variable).\n2. **Random effects**: Represent variability or random deviations associated with groups, clusters, or subjects in the data (e.g., individual-level variability).\n\n### Key Concepts\n\n1. **Fixed effects**: These are the parameters of primary interest, such as the slope and intercept in a regression model.\n2. **Random effects**: Capture the variation within clusters or groups that is not explained by the fixed effects.\n3. **Hierarchical/nested data**: LMEs are particularly useful when data are organized into groups, such as measurements repeated within subjects, or students nested within schools.\n4. **Correlation structure**: LMEs can model the correlation between observations within the same group due to shared random effects.\n\n### General Form\n\nThe model can be expressed as:\n\n$$\ny_{ij} = \\beta_0 + \\beta_1 x_{ij} + u_{j} + \\epsilon_{ij}\n$$\n\nWhere:\n\n- $y_{ij}$: Outcome variable for observation $i$ in group $j$.\n- $\\beta_0, \\beta_1$: Fixed effects (e.g., intercept and slope).\n- $x_{ij}$: Predictor variable for observation $i$ in group $j$.\n- $u_j$: Random effect for group $j$, usually assumed to follow $N(0, \\sigma_u^2)$.\n- $\\epsilon_{ij}$: Residual error, assumed to follow $N(0, \\sigma^2)$.\n\n### Example Scenario\n\n#### Research Question:\n\nDoes a new teaching method improve test scores, accounting for variability among students and schools?\n\n- **Fixed effect**: The impact of the teaching method on test scores.\n- **Random effect**: Variability in test scores due to differences between schools.\n\n#### Data Structure:\n\n- $y_{ij}$: Test scores of student $i$ in school $j$.\n- $x_{ij}$: Teaching method indicator (1 = new method, 0 = old method).\n- $u_j$: Random intercept for each school (accounts for baseline differences between schools).\n\n#### R Example\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load package\nlibrary(lme4)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'lme4' was built under R version 4.3.3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Matrix'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n\n\n:::\n\n```{.r .cell-code}\n# Simulated data\nset.seed(123)\nschool <- factor(rep(1:10, each = 10)) # 10 schools, 10 students each\nmethod <- rbinom(100, 1, 0.5) # Random teaching method assignment\ntest_score <- 50 + 5 * method + rnorm(10)[school] + rnorm(100) # Test scores\n\nschool_score_df <- data.frame(school, method, test_score)\nhead(school_score_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  school method test_score\n1      1      0   50.63296\n2      1      1   54.75100\n3      1      0   49.92011\n4      1      1   54.23474\n5      1      1   54.18153\n6      1      0   50.55685\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nschool_score_df %>% \n  mutate(method = factor(method)) %>% \n  ggplot(aes(method, test_score, color = school)) +\n  geom_point() +\n  geom_line(aes(group = school))\n```\n\n::: {.cell-output-display}\n![](lm-mixed_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit LME model\nmodel <- lmer(test_score ~ method + (1 | school), data = school_score_df)\n\n# Summary\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: test_score ~ method + (1 | school)\n   Data: school_score_df\n\nREML criterion at convergence: 293.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.04627 -0.62983 -0.03681  0.60940  2.52921 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n school   (Intercept) 0.8545   0.9244  \n Residual             0.8692   0.9323  \nNumber of obs: 100, groups:  school, 10\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  50.0003     0.3205  156.00\nmethod        5.3366     0.1972   27.07\n\nCorrelation of Fixed Effects:\n       (Intr)\nmethod -0.289\n```\n\n\n:::\n:::\n\n\n\n\n\n\n### Interpretation of R Output\n\n1. **Fixed effect for `method`**: Estimated change in test scores when using the new teaching method compared to the old method.\n2. **Random intercept for `school`**: Variability in baseline test scores across schools.\n3. **Residual variance**: Within-school variation in test scores not explained by the method or school-level effects.\n\n### Summary\n\n**Advantages of LME:**\n\n- Handles hierarchical data effectively.\n- Accounts for intra-group correlation.\n- More flexible than standard regression models.\n\n**Applications:**\n\n- Repeated measures data (e.g., patient follow-ups in clinical studies).\n- Multi-level education data (e.g., students within classrooms).\n- Longitudinal studies (e.g., growth trends over time).\n",
    "supporting": [
      "lm-mixed_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}